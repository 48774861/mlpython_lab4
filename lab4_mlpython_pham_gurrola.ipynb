{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecff4045-0946-4697-9554-e4ac6b73b1cd",
   "metadata": {},
   "source": [
    "## 1. Load, Split, and Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1271310-6935-4844-9451-ed71ad422cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize_scalar, fmin_bfgs\n",
    "from numpy.linalg import pinv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "import copy\n",
    "import time\n",
    "from numpy import ma # (masked array) this has most numpy functions that work with NaN data.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902360f6-dbb3-434b-b032-cffcaec0aeea",
   "metadata": {},
   "source": [
    "[.5 points] (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric. (2) Remove any observations that having missing data. (3) Encode any string data as integers for now. (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable. \n",
    "The next two requirements will need to be completed together as they might depend on one another:\n",
    "[.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. Should balancing of the dataset be done for both the training and testing set? Explain.\n",
    "[.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is no need to split the data multiple times for this lab.\n",
    "Note: You will need to one hot encode the target, but do not one hot encode the categorical data until instructed to do so in the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22e4b42d-e583-4a75-8f04-8a383a091c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas dataframe.\n",
    "orig_data = pd.read_csv(\"acs2017_census_tract_data.csv\")\n",
    "\n",
    "# Removed any observations that have missing data.\n",
    "cleaned_data = orig_data.dropna()\n",
    "\n",
    "# Label Encoding: Encodes all string data as integers.\n",
    "cleaned_data[['State', 'County']] = cleaned_data[['State', 'County']].apply(LabelEncoder().fit_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee992fb-8267-48aa-b5a9-7c5782de3195",
   "metadata": {},
   "source": [
    "We decided to keep the County variable because this variable could give us some insight into which specific locations may be associated with higher Child Poverty. While the State variable also gives us some location information, the County variable is more specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f43284-b6cf-40ee-961c-b6751a784cc3",
   "metadata": {},
   "source": [
    "According to a report from the U.S. Census Bureau (2023), locations are considered to be in persistent poverty if their poverty rate is above 20% for an extended period of time. As a result, we established 20% as the threshold distinguishing lower from higher poverty rates. To further categorize these rates, we aimed to separate them into four groups: low, extremely low, high, and extremely high. Looking at recent data, though a low poverty rate is less than 20%, the locations with extremely low poverty rates typically have rates less than 10%. Locations with high poverty rates often have rates between 20%and 30%; having a poverty rate above 30% signals significant economic distress and poses serious challenges for the community. As a result, we wanted our dataset to be balanced to have the same number of data representing each of these four groups: below 10%, between 10% and 20%, between 20% and 30%, and above 30%. To balance the dataset, we divide the training data into four groups based on the criteria specified above. Then, we applied random oversampling to increase the number of examples for the minority classes because we did not want to lose valuable information from the majority class by deleting examples.\n",
    "\n",
    "Sources: [Census Bureau](https://www.census.gov/newsroom/press-releases/2023/persistent-poverty.html#:~:text=MAY%209%2C%202023%20%E2%80%93%20Today%20the,or%20more%20for%2030%20years.), [Recent Data](https://www.usnews.com/news/best-states/slideshows/us-states-with-the-highest-poverty-rates#google_vignette), [30% is Dangerous](https://www.chn.org/voices/dangerous-gaps-as-inequality-rises-30-percent-of-americans-live-close-to-the-edge-and-huge-proportions-of-minorities-and-young-children-are-poor/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26132f86-1e54-48d7-b47f-3833cf126755",
   "metadata": {},
   "source": [
    "Balancing of the dataset should only be performed on the training set, not the test set. The reason for this is that the test set is intended to evaluate the model's performance on unseen data, reflecting the real-world distribution of classes. If we artificially balance the test set, it no longer represents the actual conditions that the model will face in practice. Therefore, maintaining the original distribution in the test set is crucial for accurately assessing how the model will perform in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "833c12f7-ad6d-4ef5-a967-ee559ff42099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(clean_data):\n",
    "    cleaned_data = clean_data.copy()\n",
    "    # Make a new column describing which range Child Poverty falls into.\n",
    "    cleaned_data['ChildPovertyClass'] = pd.cut(cleaned_data['ChildPoverty'], \n",
    "                                               bins=[0,10,20,30,100],\n",
    "                                               labels=[0, 1, 2, 3],\n",
    "                                              include_lowest=True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cleaned_data.drop(['ChildPoverty'], axis=1), #X\n",
    "        cleaned_data['ChildPovertyClass'], #y\n",
    "        test_size=0.2, random_state=11\n",
    "    )\n",
    "    \n",
    "    X_train_balanced = X_train.copy()\n",
    "    \n",
    "    # Oversample each class so each class has the same number of instances (matching the highest number).\n",
    "    grouped = X_train_balanced.groupby('ChildPovertyClass')\n",
    "    largest_size = grouped.size().max()\n",
    "    \n",
    "    test_data = grouped.apply(\n",
    "        lambda x: x.sample(largest_size, replace=True).reset_index(drop=True)\n",
    "    )\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    X_train = test_data.drop(['ChildPovertyClass'], axis=1)\n",
    "    X_test = X_test.drop(['ChildPovertyClass'], axis=1)\n",
    "    y_train = test_data['ChildPovertyClass']\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test);\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67c5c6-f5f7-463f-83df-d75054c7005d",
   "metadata": {},
   "source": [
    "        Alonso, here's the different datasets that you can use for Section 2 on the Two-Layer Perceptron. Only questionable data is the One-Hot Encoded and Scaled data since it has like 2,000 columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8add6801-4fd1-4fd5-b60e-421cf8a88e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Default Dataset\n",
    "\"\"\"\n",
    "X_train, y_train, X_test, y_test = balance_data(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed0b8f73-949a-4760-93bc-1ed326766d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One-Hot Encoded and Standard Scaled Data.\n",
    "\"\"\"\n",
    "# Removed any observations that have missing data.\n",
    "cleaned_data_onehot = orig_data.dropna()\n",
    "\n",
    "cleaned_data_onehot = pd.get_dummies(cleaned_data_onehot, columns=['State', 'County'],dtype=int)\n",
    "\n",
    "X_train_dummies, y_train, X_test_dummies, y_test = balance_data(cleaned_data_onehot)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_dummies = scaler.fit_transform(X_train_dummies)\n",
    "X_test_dummies = scaler.transform(X_test_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70383af7-e70b-42b4-8acd-398f628329d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Only Standard Scaled Version\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac310c3-7166-400c-928a-f44bc68a1974",
   "metadata": {},
   "source": [
    "## 2. Pre-Processing and Initial Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a1129-903f-4fb6-9161-4c08def72f38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1: Two-Layer Perceptron Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff0d1f18-b6b1-4c5a-848a-f069be31aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def print_result(nn,X_train,y_train,X_test,y_test,title=\"\",color=\"red\"):\n",
    "    \n",
    "    print(\"=================\")\n",
    "    print(title,\":\")\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "    \n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Validation acc:',accuracy_score(y_test,yhat))\n",
    "    fig = plt.figure(figsize=(6,4), dpi=200)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "\n",
    "    plt.title('Validation Accuracy vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd4bde5b-035c-4a69-8c51-c02dd25fde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLP:             \n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None,\n",
    "                 alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        # This seems to do something with the child class, which no longer exists\n",
    "        # super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                \n",
    "                \n",
    "                # simple momentum calculations\n",
    "                \n",
    "                rho_W1, rho_W2 = eta * gradW1, eta * gradW2\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                \n",
    "                # no need for momentum in bias \n",
    "                # these values need to change abruptly and \n",
    "                # do not influence sensitivity backward\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                \n",
    "                # update previous parameters \n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "                \n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "            # update if a validation set was provided\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        # set these to zero to start so that\n",
    "        # they do not immediately saturate the neurons\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3+1e-7)+(1-Y_enc)*np.log(1-A3+1e-7))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d780a5-435f-4e2e-931f-4d849d0a8a33",
   "metadata": {},
   "source": [
    "### 2.2: Comparing the Performance of the Two-Layer Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9a4968d-f502-4011-af4a-7b35739631b6",
   "metadata": {},
   "source": [
    "You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). It is recommended to use sigmoids throughout the network, but not required.\n",
    "\n",
    "[.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the feature data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "\n",
    "[.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  \n",
    "\n",
    "[.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "\n",
    "[1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a466812-7c77-48ce-84a9-0909b928ddf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20"
     ]
    }
   ],
   "source": [
    "vals = { 'n_hidden':30, \n",
    "         'C':0.1, 'epochs':20, 'eta':0.001, # poor starting learning rate!!\n",
    "         'alpha':0.001, 'decrease_const':0.1, 'decrease_iter':15,\n",
    "         'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "vals['epochs'] = 20\n",
    "vals['eta'] = 0.001\n",
    "\n",
    "nn_better = TLP(**vals)\n",
    "%time nn_better.fit(X_train_dummies, y_train, print_progress=1, XY_test=(X_test_dummies, y_test))\n",
    "\n",
    "print_result(nn_better,X_train_dummies,y_train,X_test_dummies,y_test,title=\"Two-Layer Perceptron\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43d444cc",
   "metadata": {},
   "source": [
    "\n",
    "[.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the feature data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4a9b9-2243-4d16-a3f1-788f84e076b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Default dataset without scaling or encoding\n",
    "X_train, y_train, X_test, y_test = balance_data(cleaned_data)\n",
    "\n",
    "# Initialize and train the model\n",
    "nn_default = TLP(**vals)\n",
    "%time nn_default.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "# Plot accuracy and loss for training and validation\n",
    "print_result(nn_default, X_train, y_train, X_test, y_test, title=\"Default Dataset\", color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6611df38",
   "metadata": {},
   "source": [
    "[.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2257c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Normalize numeric features only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the model on normalized numeric data only\n",
    "nn_normalized = TLP(**vals)\n",
    "%time nn_normalized.fit(X_train_scaled, y_train, print_progress=1, XY_test=(X_test_scaled, y_test))\n",
    "\n",
    "# Plot accuracy and loss for training and validation\n",
    "print_result(nn_normalized, X_train_scaled, y_train, X_test_scaled, y_test, title=\"Normalized Numeric Only\", color=\"orange\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71d39d3b",
   "metadata": {},
   "source": [
    "[.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a4f64",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model on fully preprocessed data (one-hot encoded and scaled)\n",
    "nn_better = TLP(**vals)\n",
    "%time nn_better.fit(X_train_dummies, y_train, print_progress=1, XY_test=(X_test_dummies, y_test))\n",
    "\n",
    "# Plot accuracy and loss for training and validation\n",
    "print_result(nn_better, X_train_dummies, y_train, X_test_dummies, y_test, title=\"Normalized & One-Hot Encoded\", color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b68e659d",
   "metadata": {},
   "source": [
    "[1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50771101",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get predictions for each model\n",
    "y_pred_default = nn_default.predict(X_test)\n",
    "y_pred_normalized = nn_normalized.predict(X_test_scaled)\n",
    "y_pred_onehot = nn_better.predict(X_test_dummies)\n",
    "\n",
    "# Function to calculate and print McNemar test result\n",
    "def mcnemar_test(y_true, y_pred1, y_pred2, title=\"Comparison\"):\n",
    "    # Create a contingency table\n",
    "    b = sum((y_pred1 != y_true) & (y_pred2 == y_true))\n",
    "    c = sum((y_pred1 == y_true) & (y_pred2 != y_true))\n",
    "    table = [[sum((y_pred1 == y_true) & (y_pred2 == y_true)), b],\n",
    "             [c, sum((y_pred1 != y_true) & (y_pred2 != y_true))]]\n",
    "    \n",
    "    result = mcnemar(table, exact=False)\n",
    "    print(f\"{title} - McNemar test statistic: {result.statistic}, p-value: {result.pvalue}\")\n",
    "    if result.pvalue < 0.05:\n",
    "        print(\"Significant difference between models (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"No significant difference between models (p >= 0.05)\")\n",
    "\n",
    "# Conduct McNemar tests for each pair\n",
    "mcnemar_test(y_test, y_pred_default, y_pred_normalized, title=\"Default vs. Normalized\")\n",
    "mcnemar_test(y_test, y_pred_default, y_pred_onehot, title=\"Default vs. Normalized + One-Hot\")\n",
    "mcnemar_test(y_test, y_pred_normalized, y_pred_onehot, title=\"Normalized vs. Normalized + One-Hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474995d4-e84e-4990-9701-9655e258d91b",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6d10b-5112-4ce9-aa6b-ed2853c875d5",
   "metadata": {},
   "source": [
    "[1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs.\n",
    "[1 points] Repeat the previous step, adding support for a fourth layer.\n",
    "[1 points] Repeat the previous step, adding support for a fifth layer. \n",
    "[2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (choose either RMSProp or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343adf5-7ac6-4492-a1d1-627785eb07fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Only for Testing that the newly implemented Perceptrons work, used in oroiginal notebook\n",
    "DELETE WHEN FINISHED TESTING\n",
    "\"\"\"\n",
    "# more diverse fashion MNIST data\n",
    "# Let's use Raschka's implementation for using the mnist dataset:\n",
    "# https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    " \n",
    "def load_mnist(path, kind='fashion_train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "X_train, y_train = load_mnist('data/', kind='fashion_train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist('data/', kind='fashion_t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "# important pre-processing steps!!\n",
    "X_train = X_train/255.0 - 0.5\n",
    "X_test = X_test/255.0 - 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf139b-1fda-4e67-864d-01fcb97eec72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1: Three-Layer Perceptron Code\n",
    "We added support for a third layer in the multi-layer perceptron. We have also added support for saving and plotting the magnitude of the gradient (average absolute values) after each layer for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaff1b6-fa2a-44b6-b572-954278ade1cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class ThreeLP:             \n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None,\n",
    "                 alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        # This seems to do something with the child class, which no longer exists\n",
    "        # super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.b1, self.b2, self.b3 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # added this one\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting accuracy without training.\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            # Copies the testing data (X,y) to its respective places.\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            # Gets the initial Validation Accuracy (on test data)\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.update_w1_ = np.zeros(self.epochs)\n",
    "        self.update_w2_ = np.zeros(self.epochs)\n",
    "        self.update_w3_ = np.zeros(self.epochs)\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A_last = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                        self.W3,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2,\n",
    "                                                        self.b3\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A_last,Y_enc[:, idx],self.W1,self.W2,self.W3)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradb1, gradb2, gradb3 = self._get_gradient(\n",
    "                    A1=A1, A2=A2, A3=A3, A_last=A_last, Z1=Z1, Z2=Z2, Z3=Z3,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2,W3=self.W3) # added params\n",
    "\n",
    "                # Extra stuff for Average Magnitude\n",
    "                self.grad_w1_[i] = np.mean(np.abs(gradW1))\n",
    "                self.grad_w2_[i] = np.mean(np.abs(gradW2))\n",
    "                self.grad_w3_[i] = np.mean(np.abs(gradW3))\n",
    "                \n",
    "                # simple momentum calculations\n",
    "                rho_W1, rho_W2, rho_W3 = eta * gradW1, eta * gradW2, eta * gradW3\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # added\n",
    "                \n",
    "                # no need for momentum in bias \n",
    "                # these values need to change abruptly and \n",
    "                # do not influence sensitivity backward\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3 # added\n",
    "                \n",
    "                # update previous parameters \n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev = rho_W1, rho_W2, rho_W3 # added\n",
    "\n",
    "                self.update_w1_[i] = np.mean(np.abs(eta * gradW1))\n",
    "                self.update_w2_[i] = np.mean(np.abs(eta * gradW2))\n",
    "                self.update_w3_[i] = np.mean(np.abs(eta * gradW3))\n",
    "                \n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "            # update if a validation set was provided\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "        # print(f\"Weights 1: init_bound {init_bound}, shape ({self.n_hidden}, {self.n_features_})\")\n",
    "\n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        # set these to zero to start so that\n",
    "        # they do not immediately saturate the neurons\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_hidden, 1))\n",
    "        b3 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, W3, b1, b2, b3\n",
    "    \n",
    "    def _cost(self,A_last,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A_last+1e-7)+(1-Y_enc)*np.log(1-A_last+1e-7))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, A_last, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "        # A1, Z1, A2, Z2, A3, Z3, A_last\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V3 = (A_last-Y_enc) # <- this is only line that changed\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "        gradW3 += W3 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradb1, gradb2, gradb3\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A_last,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A_last)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, W3, b1, b2, b3):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A_last = self._sigmoid(Z3)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A_last\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, A_last = self._feedforward(X, self.W1, self.W2, self.W3, self.b1, self.b2, self.b3)\n",
    "        y_pred = np.argmax(A_last, axis=0)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290459b-6192-4a5a-9735-6e40dd734b88",
   "metadata": {},
   "source": [
    "### 3.2: Testing the Performance of the Three-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0dec0f-fd26-4cd6-b812-64b9c6317c04",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "vals = { 'n_hidden':30, \n",
    "         'C':0.1, 'epochs':20, 'eta':0.001,\n",
    "         'alpha':0.001, 'decrease_const':0.1, 'decrease_iter':15,\n",
    "         'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_three = ThreeLP(**vals)\n",
    "%time nn_three.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_three,X_train,y_train,X_test,y_test,title=\"Three-Layer Perceptron\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931e53c-abb1-4830-9071-e559b1dada35",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6,4),dpi=200)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_three.update_w1_[:]), label='w1')\n",
    "plt.plot(np.abs(nn_three.update_w2_[:]), label='w2')\n",
    "plt.plot(np.abs(nn_three.update_w3_[:]), label='w3')\n",
    "plt.xticks(np.arange(0,20,2))\n",
    "plt.legend()\n",
    "plt.title(\"Three-Layer Perceptron\")\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617720f-0e7f-450e-9576-c9569f8268f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3: Four-Layer Perceptron Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45469a-1aaf-492c-831c-308e4275af68",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class FourLP:             \n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None,\n",
    "                 alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        # This seems to do something with the child class, which no longer exists\n",
    "        # super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # added this one\n",
    "        rho_W4_prev = np.zeros(self.W4.shape) # added this one\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting accuracy without training.\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            # Copies the testing data (X,y) to its respective places.\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            # Gets the initial Validation Accuracy (on test data)\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "        self.update_w1_ = np.zeros(self.epochs)\n",
    "        self.update_w2_ = np.zeros(self.epochs)\n",
    "        self.update_w3_ = np.zeros(self.epochs)\n",
    "        self.update_w4_ = np.zeros(self.epochs)\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A_last = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                        self.W3,\n",
    "                                                        self.W4,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2,\n",
    "                                                        self.b3,\n",
    "                                                        self.b4\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A_last,Y_enc[:, idx],self.W1,self.W2,self.W3,self.W4)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradW4, gradb1, gradb2, gradb3, gradb4 = self._get_gradient(\n",
    "                    A1=A1, A2=A2, A3=A3, A4=A4, A_last=A_last, \n",
    "                    Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4,\n",
    "                    Y_enc=Y_enc[:, idx],\n",
    "                    W1=self.W1,W2=self.W2,W3=self.W3,W4=self.W4) # added params\n",
    "\n",
    "                # Extra stuff for Average Magnitude\n",
    "                self.grad_w1_[i] = np.mean(np.abs(gradW1))\n",
    "                self.grad_w2_[i] = np.mean(np.abs(gradW2))\n",
    "                self.grad_w3_[i] = np.mean(np.abs(gradW3))\n",
    "                self.grad_w4_[i] = np.mean(np.abs(gradW4))\n",
    "                \n",
    "                # simple momentum calculations\n",
    "                rho_W1, rho_W2, rho_W3, rho_W4 = eta * gradW1, eta * gradW2, eta * gradW3, eta * gradW4\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # added\n",
    "                self.W4 -= (rho_W4 + (self.alpha * rho_W4_prev)) # added\n",
    "                \n",
    "                # no need for momentum in bias \n",
    "                # these values need to change abruptly and \n",
    "                # do not influence sensitivity backward\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3 # added\n",
    "                self.b4 -= eta * gradb4 # added\n",
    "                \n",
    "                # update previous parameters \n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev, rho_W4_prev = rho_W1, rho_W2, rho_W3, rho_W4 # added\n",
    "\n",
    "                self.update_w1_[i] = np.mean(np.abs(eta * gradW1))\n",
    "                self.update_w2_[i] = np.mean(np.abs(eta * gradW2))\n",
    "                self.update_w3_[i] = np.mean(np.abs(eta * gradW3))\n",
    "                self.update_w4_[i] = np.mean(np.abs(eta * gradW4))\n",
    "                \n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "            # update if a validation set was provided\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "        # print(f\"Weights 1: init_bound {init_bound}, shape ({self.n_hidden}, {self.n_features_})\")\n",
    "\n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "        \n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W4 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        # set these to zero to start so that\n",
    "        # they do not immediately saturate the neurons\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_hidden, 1))\n",
    "        b3 = np.zeros((self.n_hidden, 1))\n",
    "        b4 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, W3, W4, b1, b2, b3, b4\n",
    "    \n",
    "    def _cost(self,A_last,Y_enc,W1,W2,W3,W4):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A_last+1e-7)+(1-Y_enc)*np.log(1-A_last+1e-7))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, A4, A_last, Z1, Z2, Z3, Z4, Y_enc, W1, W2, W3, W4):\n",
    "        # A1, Z1, A2, Z2, A3, Z3, A_last\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V4 = (A_last-Y_enc) # <- this is only line that changed\n",
    "        V3 = A4*(1-A4)*(W4.T @ V4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        gradW4 = V4 @ A4.T\n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb4 = np.sum(V4, axis=1).reshape((-1,1))\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "        gradW3 += W3 * self.l2_C\n",
    "        gradW4 += W4 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradW4, gradb1, gradb2, gradb3, gradb4\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3, W4):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2) + np.mean(W4[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A_last,Y_enc,W1,W2,W3, W4):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A_last)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, W3, W4, b1, b2, b3, b4):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        Z4 = W4 @ A4 + b4\n",
    "        A_last = self._sigmoid(Z4)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A_last\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, _, _, A_last = self._feedforward(X, self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4)\n",
    "        y_pred = np.argmax(A_last, axis=0)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1559e5-f543-4291-9de1-2222cb48d102",
   "metadata": {},
   "source": [
    "### 3.4: Testing the Performance of the Four-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ca918-ef77-4c13-9eb5-191ccb80f620",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "vals = { 'n_hidden':30, \n",
    "         'C':0.1, 'epochs':20, 'eta':0.001,\n",
    "         'alpha':0.001, 'decrease_const':0.1, 'decrease_iter':15,\n",
    "         'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_four = FourLP(**vals)\n",
    "%time nn_four.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_four,X_train,y_train,X_test,y_test,title=\"Four-Layer Perceptron\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409f6cb-ad48-4457-9b6b-6b585b0df5a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6,4),dpi=200)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_four.update_w1_[:]), label='w1')\n",
    "plt.plot(np.abs(nn_four.update_w2_[:]), label='w2')\n",
    "plt.plot(np.abs(nn_four.update_w3_[:]), label='w3')\n",
    "plt.plot(np.abs(nn_four.update_w4_[:]), label='w4')\n",
    "plt.xticks(np.arange(0,20,2))\n",
    "plt.title(\"Four-Layer Perceptron\")\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb18635-1b98-4e07-8a87-de238f5ff51c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.5: Five-Layer Perceptron Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea429084-991a-4a44-831e-b994ad82e8d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class FiveLP:             \n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None,\n",
    "                 alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        # This seems to do something with the child class, which no longer exists\n",
    "        # super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.W5, self.b1, self.b2, self.b3, self.b4, self.b5 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # added this one\n",
    "        rho_W4_prev = np.zeros(self.W4.shape) # added this one\n",
    "        rho_W5_prev = np.zeros(self.W5.shape) # added this one\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting accuracy without training.\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            # Copies the testing data (X,y) to its respective places.\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            # Gets the initial Validation Accuracy (on test data)\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "        self.grad_w5_ = np.zeros(self.epochs)\n",
    "        self.update_w1_ = np.zeros(self.epochs)\n",
    "        self.update_w2_ = np.zeros(self.epochs)\n",
    "        self.update_w3_ = np.zeros(self.epochs)\n",
    "        self.update_w4_ = np.zeros(self.epochs)\n",
    "        self.update_w5_ = np.zeros(self.epochs)\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A_last = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                        self.W3,\n",
    "                                                        self.W4,\n",
    "                                                        self.W5,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2,\n",
    "                                                        self.b3,\n",
    "                                                        self.b4,\n",
    "                                                        self.b5\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A_last,Y_enc[:, idx],self.W1,self.W2,self.W3,self.W4,self.W5)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradW4, gradW5, gradb1, gradb2, gradb3, gradb4, gradb5 = self._get_gradient(\n",
    "                    A1=A1, A2=A2, A3=A3, A4=A4, A5=A5, A_last=A_last, \n",
    "                    Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4, Z5=Z5,\n",
    "                    Y_enc=Y_enc[:, idx],\n",
    "                    W1=self.W1,W2=self.W2,W3=self.W3,W4=self.W4,W5=self.W5) # added params\n",
    "\n",
    "                # Extra stuff for Average Magnitude\n",
    "                self.grad_w1_[i] = np.mean(np.abs(gradW1))\n",
    "                self.grad_w2_[i] = np.mean(np.abs(gradW2))\n",
    "                self.grad_w3_[i] = np.mean(np.abs(gradW3))\n",
    "                self.grad_w4_[i] = np.mean(np.abs(gradW4))\n",
    "                self.grad_w5_[i] = np.mean(np.abs(gradW5))\n",
    "                \n",
    "                # simple momentum calculations\n",
    "                rho_W1, rho_W2, rho_W3, rho_W4, rho_W5 = eta * gradW1, eta * gradW2, eta * gradW3, eta * gradW4, eta * gradW5\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # added\n",
    "                self.W4 -= (rho_W4 + (self.alpha * rho_W4_prev)) # added\n",
    "                self.W5 -= (rho_W5 + (self.alpha * rho_W5_prev)) # added\n",
    "                \n",
    "                # no need for momentum in bias \n",
    "                # these values need to change abruptly and \n",
    "                # do not influence sensitivity backward\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3 # added\n",
    "                self.b4 -= eta * gradb4 # added\n",
    "                self.b5 -= eta * gradb5 # added\n",
    "                \n",
    "                # update previous parameters \n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev, rho_W4_prev, rho_W5_prev = rho_W1, rho_W2, rho_W3, rho_W4, rho_W5 # added\n",
    "\n",
    "                self.update_w1_[i] = np.mean(np.abs(eta * gradW1))\n",
    "                self.update_w2_[i] = np.mean(np.abs(eta * gradW2))\n",
    "                self.update_w3_[i] = np.mean(np.abs(eta * gradW3))\n",
    "                self.update_w4_[i] = np.mean(np.abs(eta * gradW4))\n",
    "                self.update_w5_[i] = np.mean(np.abs(eta * gradW5))\n",
    "                \n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "            # update if a validation set was provided\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "        # print(f\"Weights 1: init_bound {init_bound}, shape ({self.n_hidden}, {self.n_features_})\")\n",
    "\n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "        \n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "\n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W4 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "        \n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W5 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        # set these to zero to start so that\n",
    "        # they do not immediately saturate the neurons\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_hidden, 1))\n",
    "        b3 = np.zeros((self.n_hidden, 1))\n",
    "        b4 = np.zeros((self.n_hidden, 1))\n",
    "        b5 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, W3, W4, W5, b1, b2, b3, b4, b5\n",
    "    \n",
    "    def _cost(self,A_last,Y_enc,W1,W2,W3,W4,W5):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A_last+1e-7)+(1-Y_enc)*np.log(1-A_last+1e-7))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4, W5)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5, A_last, Z1, Z2, Z3, Z4, Z5, Y_enc, W1, W2, W3, W4, W5):\n",
    "        # A1, Z1, A2, Z2, A3, Z3, A_last\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V5 = (A_last-Y_enc) # <- this is only line that changed\n",
    "        V4 = A5*(1-A5)*(W5.T @ V5)\n",
    "        V3 = A4*(1-A4)*(W4.T @ V4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        gradW5 = V5 @ A5.T\n",
    "        gradW4 = V4 @ A4.T\n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb5 = np.sum(V5, axis=1).reshape((-1,1))\n",
    "        gradb4 = np.sum(V4, axis=1).reshape((-1,1))\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "        gradW3 += W3 * self.l2_C\n",
    "        gradW4 += W4 * self.l2_C\n",
    "        gradW5 += W5 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradW4, gradW5, gradb1, gradb2, gradb3, gradb4, gradb5\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3, W4, W5):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2) + np.mean(W4[:, 1:] ** 2) + np.mean(W5[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A_last,Y_enc,W1,W2,W3, W4, W5):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A_last)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4, W5)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, W3, W4, W5, b1, b2, b3, b4, b5):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        Z4 = W4 @ A4 + b4\n",
    "        A5 = self._sigmoid(Z4)\n",
    "        Z5 = W5 @ A5 + b5\n",
    "        A_last = self._sigmoid(Z5)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A_last\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, _, _, _, _, A_last = self._feedforward(X, self.W1, self.W2, self.W3, self.W4, self.W5, self.b1, self.b2, self.b3, self.b4, self.b5)\n",
    "        y_pred = np.argmax(A_last, axis=0)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5d073-ca36-46fc-bffb-f60bf83568a7",
   "metadata": {},
   "source": [
    "### 3.6: Testing the Performance of the Five-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7db70-4f42-4a0b-9678-a3dcc04fdbdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "vals = { 'n_hidden':30, \n",
    "         'C':0.1, 'epochs':20, 'eta':0.001,\n",
    "         'alpha':0.001, 'decrease_const':0.1, 'decrease_iter':15,\n",
    "         'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_five = FiveLP(**vals)\n",
    "%time nn_five.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_five,X_train,y_train,X_test,y_test,title=\"Five-Layer Perceptron\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1dc43-be59-4ea0-b441-4cbd137a899c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6,4),dpi=200)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_five.update_w1_[:]), label='w1')\n",
    "plt.plot(np.abs(nn_five.update_w2_[:]), label='w2')\n",
    "plt.plot(np.abs(nn_five.update_w3_[:]), label='w3')\n",
    "plt.plot(np.abs(nn_five.update_w4_[:]), label='w4')\n",
    "plt.plot(np.abs(nn_five.update_w5_[:]), label='w5')\n",
    "plt.title(\"Five-Layer Perceptron\")\n",
    "plt.xticks(np.arange(0,20,2))\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccfe7a-58f5-45cf-9e32-23ec73a19f08",
   "metadata": {},
   "source": [
    "### 3.7: RMSProp + Five-Layer Perceptron\n",
    "\n",
    "Root Mean Square Propagation (RMSProp) is an adaptive learning strategy that emerged from practical use rather than formal research. Unlike other methods like Adagrad, RMSProp did not suffer from the diminishing learning rate problem since it updates its learning rate using both the current and previous gradient. This ensures that the learning rate does not drop too steeply from step to step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb685e5-8d34-40a4-9e43-bfe9d4dd600d",
   "metadata": {},
   "source": [
    "Sources: [Kaggle](https://www.kaggle.com/discussions/getting-started/455397), [Understanding RMSProp](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1ab75-5689-4b63-98a4-9d41e0fe2c3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class FiveLP_RMSProp(FiveLP):\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.W5, self.b1, self.b2, self.b3, self.b4, self.b5 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # added this one\n",
    "        rho_W4_prev = np.zeros(self.W4.shape) # added this one\n",
    "        rho_W5_prev = np.zeros(self.W5.shape) # added this one\n",
    "\n",
    "        # adaptive G for entire gradient\n",
    "        G1_prev = np.zeros(self.W1.shape) # for adaptive\n",
    "        G2_prev = np.zeros(self.W2.shape) # for adaptive\n",
    "        G3_prev = np.zeros(self.W3.shape)\n",
    "        G4_prev = np.zeros(self.W4.shape)\n",
    "        G5_prev = np.zeros(self.W5.shape)\n",
    "        \n",
    "        V1 = np.zeros(self.W1.shape)\n",
    "        V2 = np.zeros(self.W2.shape)\n",
    "        V3 = np.zeros(self.W3.shape)\n",
    "        V4 = np.zeros(self.W4.shape)\n",
    "        V5 = np.zeros(self.W5.shape)\n",
    "        \n",
    "        M1_next = np.zeros(self.W1.shape)\n",
    "        M2_next = np.zeros(self.W2.shape)\n",
    "        M3_next = np.zeros(self.W3.shape)\n",
    "        M4_next = np.zeros(self.W4.shape)\n",
    "        M5_next = np.zeros(self.W5.shape)\n",
    "        \n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting accuracy without training.\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            # Copies the testing data (X,y) to its respective places.\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            # Gets the initial Validation Accuracy (on test data)\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "        self.grad_w5_ = np.zeros(self.epochs)\n",
    "        self.update_w1_ = np.zeros(self.epochs)\n",
    "        self.update_w2_ = np.zeros(self.epochs)\n",
    "        self.update_w3_ = np.zeros(self.epochs)\n",
    "        self.update_w4_ = np.zeros(self.epochs)\n",
    "        self.update_w5_ = np.zeros(self.epochs)\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A_last = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                        self.W3,\n",
    "                                                        self.W4,\n",
    "                                                        self.W5,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2,\n",
    "                                                        self.b3,\n",
    "                                                        self.b4,\n",
    "                                                        self.b5\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A_last,Y_enc[:, idx],self.W1,self.W2,self.W3,self.W4,self.W5)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradW4, gradW5, gradb1, gradb2, gradb3, gradb4, gradb5 = self._get_gradient(\n",
    "                    A1=A1, A2=A2, A3=A3, A4=A4, A5=A5, A_last=A_last, \n",
    "                    Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4, Z5=Z5,\n",
    "                    Y_enc=Y_enc[:, idx],\n",
    "                    W1=self.W1,W2=self.W2,W3=self.W3,W4=self.W4,W5=self.W5) # added params\n",
    "\n",
    "                # Extra stuff for Average Magnitude\n",
    "                self.grad_w1_[i] = np.mean(np.abs(gradW1))\n",
    "                self.grad_w2_[i] = np.mean(np.abs(gradW2))\n",
    "                self.grad_w3_[i] = np.mean(np.abs(gradW3))\n",
    "                self.grad_w4_[i] = np.mean(np.abs(gradW4))\n",
    "                self.grad_w5_[i] = np.mean(np.abs(gradW5))\n",
    "\n",
    "                \"\"\"\n",
    "                Beginning of Changes\n",
    "                \"\"\"\n",
    "                # momentum and adaptation calculations\n",
    "                # Eta-grad implementation:\n",
    "                # M_next = 0 at the start\n",
    "        \n",
    "                # Update for W1\n",
    "                V1 = self.alpha * V1 + (1 - self.alpha) * gradW1 * gradW1\n",
    "                rho_W1 = eta / np.sqrt(V1 + 1e-7) * gradW1\n",
    "                \n",
    "                # Update for W2\n",
    "                V2 = self.alpha * V2 + (1 - self.alpha) * gradW2 * gradW2\n",
    "                rho_W2 = eta / np.sqrt(V2 + 1e-7) * gradW2\n",
    "                \n",
    "                # Update for W3\n",
    "                V3 = self.alpha * V3 + (1 - self.alpha) * gradW3 * gradW3\n",
    "                rho_W3 = eta / np.sqrt(V3 + 1e-7) * gradW3\n",
    "                \n",
    "                # Update for W4\n",
    "                V4 = self.alpha * V4 + (1 - self.alpha) * gradW4 * gradW4\n",
    "                rho_W4 = eta / np.sqrt(V4 + 1e-7) * gradW4\n",
    "                \n",
    "                # Update for W5\n",
    "                V5 = self.alpha * V5 + (1 - self.alpha) * gradW5 * gradW5\n",
    "                rho_W5 = eta / np.sqrt(V5 + 1e-7) * gradW5\n",
    "        \n",
    "                # G1 = gradW1*gradW1 + self.alpha*G1_prev + 1e-7\n",
    "                # G2 = gradW2*gradW2 + self.alpha*G2_prev + 1e-7\n",
    "                # G3 = gradW3*gradW3 + self.alpha*G3_prev + 1e-7\n",
    "                # G4 = gradW4*gradW4 + self.alpha*G4_prev + 1e-7\n",
    "                # G5 = gradW5*gradW5 + self.alpha*G5_prev + 1e-7\n",
    "                \n",
    "                # # momentum and adagrad\n",
    "                # rho_W1 = eta * gradW1/np.sqrt(G1)\n",
    "                # rho_W2 = eta * gradW2/np.sqrt(G2)\n",
    "                # rho_W3 = eta * gradW3/np.sqrt(G3)\n",
    "                # rho_W4 = eta * gradW4/np.sqrt(G4)\n",
    "                # rho_W5 = eta * gradW5/np.sqrt(G5)\n",
    "\n",
    "                \"\"\"\n",
    "                End of Changes\n",
    "                \"\"\"\n",
    "\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # added\n",
    "                self.W4 -= (rho_W4 + (self.alpha * rho_W4_prev)) # added\n",
    "                self.W5 -= (rho_W5 + (self.alpha * rho_W5_prev)) # added\n",
    "                \n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3 # added\n",
    "                self.b4 -= eta * gradb4 # added\n",
    "                self.b5 -= eta * gradb5 # added\n",
    "                \n",
    "                # update previous parameters \n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev, rho_W4_prev, rho_W5_prev = rho_W1, rho_W2, rho_W3, rho_W4, rho_W5 # added\n",
    "                # G1_prev, G2_prev, G3_prev, G4_prev, G5_prev = G1, G2, G3, G4, G5\n",
    "                # V1_prev, V2_prev, V3_prev, V4_prev, V5_prev = V1, V2, V3, V4, V5\n",
    "\n",
    "\n",
    "                self.update_w1_[i] = np.mean(np.abs(eta * gradW1))\n",
    "                self.update_w2_[i] = np.mean(np.abs(eta * gradW2))\n",
    "                self.update_w3_[i] = np.mean(np.abs(eta * gradW3))\n",
    "                self.update_w4_[i] = np.mean(np.abs(eta * gradW4))\n",
    "                self.update_w5_[i] = np.mean(np.abs(eta * gradW5))\n",
    "                \n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "            # update if a validation set was provided\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51ce42f-7f34-4149-b2e9-8ad1aefeaa10",
   "metadata": {},
   "source": [
    "### 3.8: Testing the Performance of RMSProp + Five-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f77212-1d36-4162-8f9a-52c010efd3e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "vals = {'n_hidden':50, \n",
    "         'C':1e-2, 'epochs':20, 'eta':0.005, \n",
    "         'alpha':0.1, 'decrease_const':0.1,\n",
    "         'decrease_iter':20,\n",
    "         'minibatches':len(X_train)/256,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_five_rms = FiveLP_RMSProp(**vals)\n",
    "%time nn_five_rms.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_five_rms,X_train,y_train,X_test,y_test,title=\"Five-Layer Perceptron + RMS Prop\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284a8f7-c474-41a3-b64b-0e4686379829",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6,4),dpi=200)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_five_rms.update_w1_[:]), label='w1')\n",
    "plt.plot(np.abs(nn_five_rms.update_w2_[:]), label='w2')\n",
    "plt.plot(np.abs(nn_five_rms.update_w3_[:]), label='w3')\n",
    "plt.plot(np.abs(nn_five_rms.update_w4_[:]), label='w4')\n",
    "plt.plot(np.abs(nn_five_rms.update_w5_[:]), label='w5')\n",
    "plt.title(\"Five-Layer Perceptron + RMSProp\")\n",
    "plt.xticks(np.arange(0,20,2))\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ebeba-4bbf-4a52-a1ec-03b9e3c72c16",
   "metadata": {},
   "source": [
    "## 4. Extra Work: Six-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da510998-642b-4b1b-901e-2bbfe7127e7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.1: Six-Layer Perceptron Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c0bf4-314f-439e-9baf-2b37fdd1320e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class SixLP:             \n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None,\n",
    "                 alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        # This seems to do something with the child class, which no longer exists\n",
    "        # super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.W5, self.W6, self.b1, self.b2, self.b3, self.b4, self.b5, self.b6 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # added this one\n",
    "        rho_W4_prev = np.zeros(self.W4.shape) # added this one\n",
    "        rho_W5_prev = np.zeros(self.W5.shape) # added this one\n",
    "        rho_W6_prev = np.zeros(self.W6.shape) # added this one\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting accuracy without training.\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            # Copies the testing data (X,y) to its respective places.\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            # Gets the initial Validation Accuracy (on test data)\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "        self.grad_w5_ = np.zeros(self.epochs)\n",
    "        self.grad_w6_ = np.zeros(self.epochs)\n",
    "        self.update_w1_ = np.zeros(self.epochs)\n",
    "        self.update_w2_ = np.zeros(self.epochs)\n",
    "        self.update_w3_ = np.zeros(self.epochs)\n",
    "        self.update_w4_ = np.zeros(self.epochs)\n",
    "        self.update_w5_ = np.zeros(self.epochs)\n",
    "        self.update_w6_ = np.zeros(self.epochs)\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6, Z6, A_last = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                        self.W3,\n",
    "                                                        self.W4,\n",
    "                                                        self.W5,\n",
    "                                                        self.W6,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2,\n",
    "                                                        self.b3,\n",
    "                                                        self.b4,\n",
    "                                                        self.b5,\n",
    "                                                        self.b6\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A_last,Y_enc[:, idx],self.W1,self.W2,self.W3,self.W4,self.W5,self.W6)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradW4, gradW5, gradW6, gradb1, gradb2, gradb3, gradb4, gradb5, gradb6 = self._get_gradient(\n",
    "                    A1=A1, A2=A2, A3=A3, A4=A4, A5=A5, A6=A6, A_last=A_last, \n",
    "                    Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4, Z5=Z5, Z6=Z6,\n",
    "                    Y_enc=Y_enc[:, idx],\n",
    "                    W1=self.W1,W2=self.W2,W3=self.W3,W4=self.W4,W5=self.W5,W6=self.W6) # added params\n",
    "\n",
    "                # Extra stuff for Average Magnitude\n",
    "                self.grad_w1_[i] = np.mean(np.abs(gradW1))\n",
    "                self.grad_w2_[i] = np.mean(np.abs(gradW2))\n",
    "                self.grad_w3_[i] = np.mean(np.abs(gradW3))\n",
    "                self.grad_w4_[i] = np.mean(np.abs(gradW4))\n",
    "                self.grad_w5_[i] = np.mean(np.abs(gradW5))\n",
    "                self.grad_w6_[i] = np.mean(np.abs(gradW6))\n",
    "                \n",
    "                # simple momentum calculations\n",
    "                rho_W1, rho_W2, rho_W3, rho_W4, rho_W5, rho_W6 = eta * gradW1, eta * gradW2, eta * gradW3, eta * gradW4, eta * gradW5, eta * gradW6\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # added\n",
    "                self.W4 -= (rho_W4 + (self.alpha * rho_W4_prev)) # added\n",
    "                self.W5 -= (rho_W5 + (self.alpha * rho_W5_prev)) # added\n",
    "                self.W6 -= (rho_W6 + (self.alpha * rho_W6_prev)) # added\n",
    "                \n",
    "                # no need for momentum in bias \n",
    "                # these values need to change abruptly and \n",
    "                # do not influence sensitivity backward\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3 # added\n",
    "                self.b4 -= eta * gradb4 # added\n",
    "                self.b5 -= eta * gradb5 # added\n",
    "                self.b6 -= eta * gradb6 # added\n",
    "                \n",
    "                # update previous parameters \n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev, rho_W4_prev, rho_W5_prev, rho_W6_prev = rho_W1, rho_W2, rho_W3, rho_W4, rho_W5, rho_W6 # added\n",
    "\n",
    "                self.update_w1_[i] = np.mean(np.abs(eta * gradW1))\n",
    "                self.update_w2_[i] = np.mean(np.abs(eta * gradW2))\n",
    "                self.update_w3_[i] = np.mean(np.abs(eta * gradW3))\n",
    "                self.update_w4_[i] = np.mean(np.abs(eta * gradW4))\n",
    "                self.update_w5_[i] = np.mean(np.abs(eta * gradW5))\n",
    "                self.update_w6_[i] = np.mean(np.abs(eta * gradW6))\n",
    "                \n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "            # update if a validation set was provided\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "        # print(f\"Weights 1: init_bound {init_bound}, shape ({self.n_hidden}, {self.n_features_})\")\n",
    "\n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "        \n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "\n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W4 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "\n",
    "        # Assuming we keep the number of neurons the same for each hidden layer.\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_hidden))\n",
    "        W5 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_hidden))\n",
    "        \n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W6 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        # set these to zero to start so that\n",
    "        # they do not immediately saturate the neurons\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_hidden, 1))\n",
    "        b3 = np.zeros((self.n_hidden, 1))\n",
    "        b4 = np.zeros((self.n_hidden, 1))\n",
    "        b5 = np.zeros((self.n_hidden, 1))\n",
    "        b6 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, W3, W4, W5, W6, b1, b2, b3, b4, b5, b6\n",
    "    \n",
    "    def _cost(self,A_last,Y_enc,W1,W2,W3,W4,W5,W6):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A_last+1e-7)+(1-Y_enc)*np.log(1-A_last+1e-7))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4, W5, W6)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5, A6, A_last, Z1, Z2, Z3, Z4, Z5, Z6, Y_enc, W1, W2, W3, W4, W5, W6):\n",
    "        # A1, Z1, A2, Z2, A3, Z3, A_last\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V6 = (A_last-Y_enc) # <- this is only line that changed\n",
    "        V5 = A6*(1-A6)*(W6.T @ V6)\n",
    "        V4 = A5*(1-A5)*(W5.T @ V5)\n",
    "        V3 = A4*(1-A4)*(W4.T @ V4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        gradW6 = V6 @ A6.T\n",
    "        gradW5 = V5 @ A5.T\n",
    "        gradW4 = V4 @ A4.T\n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb6 = np.sum(V6, axis=1).reshape((-1,1))\n",
    "        gradb5 = np.sum(V5, axis=1).reshape((-1,1))\n",
    "        gradb4 = np.sum(V4, axis=1).reshape((-1,1))\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "        gradW3 += W3 * self.l2_C\n",
    "        gradW4 += W4 * self.l2_C\n",
    "        gradW5 += W5 * self.l2_C\n",
    "        gradW6 += W6 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradW4, gradW5, gradW6, gradb1, gradb2, gradb3, gradb4, gradb5, gradb6\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3, W4, W5, W6):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2) + np.mean(W4[:, 1:] ** 2) + np.mean(W5[:, 1:] ** 2) + np.mean(W6[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A_last,Y_enc,W1,W2,W3, W4, W5, W6):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A_last)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4, W5, W6)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, W3, W4, W5, W6, b1, b2, b3, b4, b5, b6):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        Z4 = W4 @ A4 + b4\n",
    "        A5 = self._sigmoid(Z4)\n",
    "        Z5 = W5 @ A5 + b5\n",
    "        A6 = self._sigmoid(Z5)\n",
    "        Z6 = W6 @ A6 + b6\n",
    "        A_last = self._sigmoid(Z6)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6, Z6, A_last\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, _, _, _, _, _, _, A_last = self._feedforward(X, self.W1, self.W2, self.W3, self.W4, self.W5, self.W6, self.b1, self.b2, self.b3, self.b4, self.b5, self.b6)\n",
    "        y_pred = np.argmax(A_last, axis=0)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf919c66-65e1-4fc3-8c3c-c78c61f3aca4",
   "metadata": {},
   "source": [
    "### 4.2: Testing the Performance of the Six-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ae441-6266-403e-a7c9-bb9bdd7bb91d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "vals = { 'n_hidden':30, \n",
    "         'C':0.1, 'epochs':20, 'eta':0.001,\n",
    "         'alpha':0.001, 'decrease_const':0.1, 'decrease_iter':15,\n",
    "         'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_six = SixLP(**vals)\n",
    "%time nn_six.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_six,X_train,y_train,X_test,y_test,title=\"Six-Layer Perceptron\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94990620-c2bc-41ca-a364-87d8db6fdeee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6,4),dpi=200)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_six.update_w1_[:]), label='w1')\n",
    "plt.plot(np.abs(nn_six.update_w2_[:]), label='w2')\n",
    "plt.plot(np.abs(nn_six.update_w3_[:]), label='w3')\n",
    "plt.plot(np.abs(nn_six.update_w4_[:]), label='w4')\n",
    "plt.plot(np.abs(nn_six.update_w5_[:]), label='w5')\n",
    "plt.plot(np.abs(nn_six.update_w6_[:]), label='w6')\n",
    "plt.xticks(np.arange(0,20,2))\n",
    "plt.legend()\n",
    "plt.title(\"Six-Layer Perceptron\")\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0369f-5f51-4e23-b284-86ded5cb02ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d6c79a3f9bf9e863186aca867b62738464a94ab2dccb66d84f4af4e48e75bbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
