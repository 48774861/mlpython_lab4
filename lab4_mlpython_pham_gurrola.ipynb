{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a41a25e-9198-4b87-a604-0b58a51e6770",
   "metadata": {},
   "source": [
    "# Lab Assignment Four: Multi-Layer Perceptron\n",
    "Name: Marc Pham, Alonso Gurrola"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff4045-0946-4697-9554-e4ac6b73b1cd",
   "metadata": {},
   "source": [
    "## 1. Load, Split, and Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1271310-6935-4844-9451-ed71ad422cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize_scalar, fmin_bfgs\n",
    "from numpy.linalg import pinv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "import copy\n",
    "import time\n",
    "from numpy import ma # (masked array) this has most numpy functions that work with NaN data.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e4b42d-e583-4a75-8f04-8a383a091c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        20.8\n",
       "1        35.8\n",
       "2        21.1\n",
       "3         1.7\n",
       "4        17.9\n",
       "         ... \n",
       "73996    61.8\n",
       "73997    39.9\n",
       "73998    77.2\n",
       "73999    58.0\n",
       "74000    72.2\n",
       "Name: ChildPoverty, Length: 74001, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_data = pd.read_csv(\"acs2017_census_tract_data.csv\")\n",
    "orig_data[\"ChildPoverty\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e23b54-c1f8-42ec-9239-4e6052dae9da",
   "metadata": {},
   "source": [
    "Sure! Let’s walk through an example of balancing a training set using quantization, specifically for a continuous variable.\n",
    "\n",
    "Example Scenario\n",
    "Dataset Overview:\n",
    "\n",
    "Suppose you have a dataset with a continuous variable, \"ChildPoverty,\" that ranges from 0 to 100, and you want to create four balanced classes.\n",
    "Steps to Balance the Training Set Using Quantization\n",
    "Identify the Continuous Variable:\n",
    "\n",
    "ChildPoverty values range from 0 to 100.\n",
    "Define Quantization Thresholds:\n",
    "\n",
    "Divide the range into four equal intervals (quartiles):\n",
    "Class 0: 0 to 25\n",
    "Class 1: 25 to 50\n",
    "Class 2: 50 to 75\n",
    "Class 3: 75 to 100\n",
    "Assign Classes Based on Thresholds:\n",
    "\n",
    "For each instance in the dataset, assign a class based on its \"ChildPoverty\" value:\n",
    "Values between 0 and 25 → Class 0\n",
    "Values between 25 and 50 → Class 1\n",
    "Values between 50 and 75 → Class 2\n",
    "Values between 75 and 100 → Class 3\n",
    "Count Instances in Each Class:\n",
    "\n",
    "After assigning classes, you might find:\n",
    "Class 0: 200 instances\n",
    "Class 1: 300 instances\n",
    "Class 2: 500 instances\n",
    "Class 3: 100 instances\n",
    "Balancing the Training Set:\n",
    "\n",
    "In this example, Class 3 (the highest poverty level) is the smallest class.\n",
    "To balance the training set, you can use Random Oversampling:\n",
    "Randomly duplicate instances from Class 3 until it has the same number of instances as the largest class (Class 2).\n",
    "For instance, if Class 2 has 500 instances, you will duplicate instances in Class 3 to also reach 500:\n",
    "\n",
    "Class 3: 100 original + 400 duplicates = 500 instances\n",
    "Create the Balanced Training Set:\n",
    "\n",
    "After oversampling, the training set will look like this:\n",
    "Class 0: 200 instances\n",
    "Class 1: 300 instances\n",
    "Class 2: 500 instances\n",
    "Class 3: 500 instances\n",
    "Separate the Test Set:\n",
    "\n",
    "Let’s say the **test set** originally had the same distribution:\n",
    "Class 0: 50 instances\n",
    "Class 1: 80 instances\n",
    "Class 2: 120 instances\n",
    "Class 3: 20 instances\n",
    "**Keep this distribution unchanged for evaluation.**\n",
    "Summary of the Balancing Process\n",
    "Before Balancing (Training Set):\n",
    "\n",
    "Class 0: 200\n",
    "Class 1: 300\n",
    "Class 2: 500\n",
    "Class 3: 100\n",
    "After Balancing (Training Set):\n",
    "\n",
    "**The Example only does this for Class 3, but we should do it for Class 0 and Class 1 as well**\n",
    "Class 0: 200\n",
    "Class 1: 300\n",
    "Class 2: 500\n",
    "Class 3: 500\n",
    "Test Set (Unchanged):\n",
    "\n",
    "Class 0: 50\n",
    "Class 1: 80\n",
    "Class 2: 120\n",
    "Class 3: 20\n",
    "Conclusion\n",
    "In this example, we used quantization to convert a continuous variable into discrete classes and then balanced the training set using random oversampling for the minority class. The test set remains unchanged to ensure that model performance can be evaluated against the original data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac310c3-7166-400c-928a-f44bc68a1974",
   "metadata": {},
   "source": [
    "## 2. Pre-Processing and Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4a9b9-2243-4d16-a3f1-788f84e076b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "474995d4-e84e-4990-9701-9655e258d91b",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343adf5-7ac6-4492-a1d1-627785eb07fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# more diverse fashion MNIST data\n",
    "# Let's use Raschka's implementation for using the mnist dataset:\n",
    "# https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    " \n",
    "def load_mnist(path, kind='fashion_train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "X_train, y_train = load_mnist('data/', kind='fashion_train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist('data/', kind='fashion_t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "# important pre-processing steps!!\n",
    "X_train = X_train/255.0 - 0.5\n",
    "X_test = X_test/255.0 - 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bde5b-035c-4a69-8c51-c02dd25fde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLPBetterInitial(TLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        # set these to zero to start so that\n",
    "        # they do not immediately saturate the neurons\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b414e5-8b9b-47da-b211-e118bfe9c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = {'n_hidden':50, \n",
    "         'C':1e-2, 'epochs':75, 'eta':0.005, \n",
    "         'alpha':0.1, 'decrease_const':0.1,\n",
    "         'decrease_iter':20,\n",
    "         'minibatches':len(X_train)/256,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_long_sigmoid = TLPBetterInitial(**vals)\n",
    "%time nn_long_sigmoid.fit(X_train, y_train, print_progress=1, XY_test=(X_test,y_test))\n",
    "print_result(nn_long_sigmoid,X_train,y_train,X_test,y_test,title=\"Long Run\",color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ebeba-4bbf-4a52-a1ec-03b9e3c72c16",
   "metadata": {},
   "source": [
    "## 4. Extra Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0369f-5f51-4e23-b284-86ded5cb02ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
